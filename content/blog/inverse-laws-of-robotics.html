<!-- date: 2026-01-12 -->
<!-- title: Inverse Laws of Robotics -->
<!-- tag: Miscellaneous -->
<!-- key: spilr -->
<p>
  Since the launch of ChatGPT in November 2022, generative artificial
  intelligence (AI) chatbot services have become increasingly
  sophisticated and popular.  These systems are now embedded in search
  engines, office software and developer tools, and for many people
  they have quickly become part of everyday computing.
</p>
<p>
  I personally find these services quite useful, particularly for
  exploring unfamiliar topics and as a general productivity aid.
  However, I also think that the way these services are advertised and
  consumed can pose a danger, especially if we get into the habit of
  trusting their output without further scrutiny.  For example, many
  popular search engines are already highlighting answers generated by
  AI at the very top of the page.  When this happens, it is easy to
  stop scrolling, accept the generated answer and move on.  Over time,
  this could inadvertently train users to treat AI as the default
  authority rather than as a starting point for further investigation.
  I wish that each such generative AI service came with a short but
  conspicuous disclaimer explaining that these systems can be wrong,
  misleading or incomplete, and that habitual trusting in their output
  can be dangerous.  In my experience, even when disclaimers exist,
  they tend to be minimal and visually deemphasised.
</p>
<p>
  In the world of science fiction, there are the
  <a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics">Three
  Laws of Robotics</a> devised by Isaac Asimov, which he used
  throughout much of his work.  These laws were designed to constrain
  the behaviour of robots in order to keep humans safe.  As far as I
  know, Asimov never formulated any equivalent laws governing how
  humans should interact with robots.  I think we now need something
  to that effect to keep ourselves safe.  I will call them
  the <em>Inverse Laws of Robotics</em>.  This applies to any
  situation that requires us humans to interact with a robot, where
  a <em>robot</em> refers to any machine, computer program or software
  service capable of performing complex tasks automatically.
</p>
<p>
  I propose the following three inverse laws of robotics:
</p>
<ol>
  <li>
    Humans must not anthropomorphise AI systems.
  </li>
  <li>
    Humans must not blindly trust the output of AI systems.
  </li>
  <li>
    Humans must remain fully responsible and accountable for
    consequences arising from the use of AI.
  </li>
</ol>
<h2 id="non-anthromorphism">Non-Anthropomorphism</h2>
<p>
  Humans must not anthropomorphise AI systems.  That is, humans must
  not attribute emotions, intentions, consciousness or moral agency to
  them.  Anthropomorphism distorts judgement.  In extreme cases,
  anthropomorphising can lead to emotional dependence.
</p>
<p>
  Modern chatbot systems often sound conversational and empathetic.
  They use polite phrasing and social cues that closely resemble human
  interaction.  While this makes them easier and more pleasant to use,
  it also makes it easier to forget what they actually are: large
  statistical models producing plausible text based on patterns in
  data.
</p>
<p>
  I think vendors of AI based chatbot services could do a better job
  here.  In many cases, the systems are deliberately tuned to feel more
  human rather than more mechanical.  I would argue that the opposite
  approach would be healthier in the long term.  A slightly more
  robotic tone would reduce the likelihood that users mistake fluent
  language for understanding, judgement or intent.
</p>
<p>
  Whether or not vendors make such changes, the responsibility for
  avoiding this pitfall still lies with users.  We must actively avoid
  the habit of treating AI systems as social actors or moral agents.
  Doing so preserves clear thinking about their capabilities and
  limitations.
</p>
<h2 id="non-deference">Non-Deference</h2>
<p>
  Humans must not blindly trust the output of AI systems.
  AI-generated content must not be treated as authoritative without
  independent verification appropriate to its context.
</p>
<p>
  This principle is not unique to AI.  In most areas of life, we should
  not accept information uncritically.  In practice, of course, this
  is not always feasible.  Not everyone is an expert in medicine,
  engineering or law, so we often rely on trusted institutions,
  professionals or public health authorities for guidance.
</p>
<p>
  Although AI systems today have become quite impressive at certain
  tasks, they are still known to produce output that would be a
  mistake to rely on.  Even if AI systems improve to the point of
  producing reliable output with a high degree of likelihood, due to
  their inherent stochastic nature, there would still be a small
  likelihood of producing output that contains errors.  This makes
  them particularly dangerous when used in contexts where errors are
  subtle but costly.  The more serious the potential consequences, the
  higher the burden of verification should be.
</p>
<h2 id="human-accountability">Human Accountability</h2>
<p>
  Humans must remain fully responsible and accountable for
  consequences arising from the use of AI.  If a negative outcome
  occurs as a result of following AI-generated advice or decisions, it
  is not sufficient to say, 'the AI told us to do it'.
</p>
<p>
  AI systems do not choose goals, deploy themselves or bear the costs
  of failure.  Humans and organisations do.  An AI system is a tool,
  and like any other tool, responsibility for its use rests with the
  people who decide to rely on it.
</p>
<p>
  This is easier said than done, though.  It gets especially tricky in
  real-time applications like self-driving cars, where a human does
  not have the opportunity to sufficiently review the decisions taken
  by the AI system before it acts.  Requiring a human driver to remain
  constantly vigilant does not solve the problem that the AI system
  often acts faster than a person can reliably intervene.  However, if
  an AI system fails in such applications, it goes without saying that
  the responsibility for investigating the failure and adding
  additional guardrails should still fall on the humans responsible
  for the design of the system.
</p>
<p>
  In all other cases, where there is no physical constraint that
  prevents a human from reviewing the AI output before it is acted
  upon, any negative consequence arising from the use of AI must fall
  entirely on the human decision-maker.  As a general principle, we
  should never accept 'the AI told us so' as a valid explanation for
  harmful outcomes.  Yes, the AI may have produced the recommendation
  but a human decided to follow it, so that human must be held
  responsible.  This is absolutely critical to preventing the
  indiscriminate use of AI in situations where irresponsible use can
  cause significant harm.
</p>
<h2 id="conclusion">Conclusion</h2>
<p>
  The three laws outlined above are based on usage patterns I have
  seen that I feel are detrimental to society.  I am hoping that with
  these three simple laws, we can encourage our fellow inhabitants on
  this blue planet to pause and reflect on how they interact with
  modern AI systems, to resist habits that weaken judgement or blur
  responsibility and to remain mindful that AI is a tool we choose to
  use, not an authority we defer to.
</p>
