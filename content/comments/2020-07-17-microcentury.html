<!-- date: 2020-07-18 10:59:51 +0000 -->
<!-- name: Ivo -->
<p>
If we consider optimal attention span, a microcentury is much two long.
Two half microcenturies with a pause in between would be better.
</p>
<!-- date: 2020-07-18 19:07:59 +0000 -->
<!-- name: Hillel Wayne -->
<!-- url: https://twitter.com/Hillelogram -->
<!-- source: https://lobste.rs/s/rsj1ov/microcentury#c_0k421w -->
<p>
I like this analysis a lot! But there's one more twist to the story: are
we even using the right definition of year? There's actually several
different ways we can define a year:
</p>
<ul>
<li>
  NIST-811 defines the light-year "year" as 365.2500 days and the
  "common year" as 365 days exactly.
</li>
<li>
  The sidereal year: the time it takes for the stars to return to a
  fixed point. That's 365.2563 days.
</li>
<li>
  The tropical year: the time between vernal equinoxes. That's 365.2421
  days.
</li>
<li>
  The anomalistic year: the time between points where the earth is
  closest to the sun. That's 365.2596 days.
</li>
<p>
For each of these we can define the microcentury as a hundred
microyears. So the sidereal microcentury is +35.81 seconds, while the
tropical microcentury is +35.69 seconds. The NIST microcentury is +33.6
seconds, while the Julian microcentury is +35.76 seconds (as you
calculated).
</p>
<p>
Personally, I'd define a calendar microcentury as a four-millionth of
400 years to match the leap year cycle. That'd give us +35.692 seconds.
</p>
<!-- date: 2020-11-20 07:26:37 +0000 -->
<!-- name: Fran&ccedil;ois Best -->
<!-- url: https://francoisbest.com -->
<p>
We had a running joke in engineering school, that &pi; seconds is close
to a nanocentury by around 6 nanomonths, we obviously didn't go that far
in calculations, so I wonder if it still holds.
</p>
